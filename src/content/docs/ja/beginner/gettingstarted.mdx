---
title: RLMatrixを始めよう
description: C#での強化学習に関する初心者向け入門ガイド
---
import Quiz from '@/components/Quiz.astro';

:::tip[強化学習の経験者ですか？]
既に強化学習の概念に精通している場合は、より速くセットアップするための[クイックスタートガイド](../quickstart/setup)をご覧ください。
:::

## はじめに

従来のプログラミングでは、あらゆる状況で何をすべきかをコンピュータに正確に指示します。例えば、数字を一致させるプログラムを書きたい場合は、次のように書くでしょう：

```csharp
if (input == pattern)
{
    return "正解！";
}
else 
{
    return "もう一度試してください！";
}
```

しかし、プログラムに自ら学習させたい場合はどうでしょうか？ルールが複雑すぎて書き出せない場合や、自分自身もルールを完全に理解していない場合はどうでしょうか？ここで強化学習の出番です。

:::note[強化学習とは？]
<details>
    <summary>簡単な説明</summary>

    新しいビデオゲームをどのように学ぶか考えてみましょう：
    1. まず何が起こるか見るためにコントロールを試します
    2. ゲームがどう反応するかを観察します
    3. ポイントを獲得したり、ライフを失ったりします
    4. 時間をかけて、何が最も効果的かを学びます

    強化学習も同じパターンに従います：
    1. AI（「エージェント」と呼びます）は様々な行動を試します
    2. 環境での結果を観察します
    3. 報酬やペナルティを受け取ります
    4. 時間をかけて、どの行動が最も効果的かを学習します

    AIに何をすべきか正確に指示する人はいません - 試行錯誤を通じて自ら解決策を見つけ出します。
</details>
:::

## プロジェクトのセットアップ
このガイドに沿って進めるか、[GitHubリポジトリ](https:/www.github.com/RLMatrix/RLMatrix-Example)をクローンしてください。
まず、必要なものをインストールしましょう：

```bash title="NuGet経由でRLMatrixをインストール"
dotnet add package RLMatrix
dotnet add package RLMatrix.Toolkit
```

:::caution[ハードウェア要件]
RLMatrixはNVIDIA GPUを搭載したWindowsパソコンでのみテストされています。ただし、これは必須ではなく、多くのユースケースではCPUでのトレーニングや推論でも十分、あるいはより高速な場合もあります。

互換性のあるグラフィックカードをお持ちでない場合は：
1. [RLMatrixリポジトリ](https://github.com/asieradzk/RL_Matrix)からソースコードを入手
2. CPU使用に変更（RLMatrix.csprojの`TorchSharp-cuda-windows`を探してください）
3. ご自身でビルドしてください
:::

## 最初の学習環境

シンプルながらも意味のある環境を作りましょう - AIがパターンを一致させることを学習する環境です。これは基本的に見えますが（そして直接プログラムするのは簡単ですが）、必要な主要な概念をすべて紹介します。

:::note[基本要素]
<details>
    <summary>重要な用語の説明</summary>

    詳細に入る前に、いくつかの重要な用語を理解しましょう：

    - **環境**：AIが存在する世界。ゲームボードやシミュレーションのようなものです。

    - **状態/観測**：AIが環境について見たり知ったりできること。
    例：一致させる必要があるパターン。

    - **行動**：AIができること。
    例：数字を選ぶこと。

    - **報酬**：AIの行動がどれだけ良かったかを伝えるフィードバック。
    例：正しい一致で+1、間違った一致で-1。

    - **エピソード**：タスクの1回の完全な試行。
    ゲームの1ラウンドだと考えてください。

</details>
:::

こちらが完全な環境です：

```csharp title="PatternMatchingEnvironment.cs"
using RLMatrix.Toolkit;

namespace PatternMatchingExample;

[RLMatrixEnvironment]
public partial class PatternMatchingEnvironment
{
    private int pattern = 0;
    private int aiChoice = 0;
    private bool roundFinished = false;

    // 直近50ステップ用の簡単なカウンター
    private int correct = 0;
    private int total = 0;

    // 正確さの簡単な計算
    public float RecentAccuracy => total > 0 ? (float)correct / total * 100 : 0;

    [RLMatrixObservation]
    public float SeePattern() => pattern;

    [RLMatrixActionDiscrete(2)]
    public void MakeChoice(int choice)
    {
        aiChoice = choice;
        roundFinished = true;

        // カウンターの更新
        total++;
        if (aiChoice == pattern) correct++;
    }

    [RLMatrixReward]
    public float GiveReward() => aiChoice == pattern ? 1.0f : -1.0f;

    [RLMatrixDone]
    public bool IsRoundOver() => roundFinished;

    [RLMatrixReset]
    public void StartNewRound()
    {
        pattern = Random.Shared.Next(2);
        aiChoice = 0;
        roundFinished = false;
    }

    public void ResetStats()
    {
        correct = 0;
        total = 0;
    }
}
```

:::note[コードの理解]
<details>
    <summary>コードの内訳</summary>

    各部分を見てみましょう：

    **変数：**
    ```csharp
    private int pattern = 0;      // 一致させる数字
    private int aiChoice = 0;     // AIの予測
    private bool roundFinished = false;  // ラウンドのステータス
    ```
    これらは環境で何が起こっているかを追跡します。

    **特殊な属性：**
    - `[RLMatrixEnvironment]`：RLMatrixに「これは学習環境です」と伝えます
    - `[RLMatrixObservation]`：「これはAIが見ることができるものです」
    - `[RLMatrixActionDiscrete]`：「これらはAIが行える選択です」
    - `[RLMatrixReward]`：「これはAIのパフォーマンスをスコア化する方法です」
    - `[RLMatrixReset]`：「これは新しく始める方法です」

    ツールキットはこれらの属性を使用して必要なコードを自動的に生成します。
</details>
:::

## AIのトレーニング

ここから面白い部分が始まります - AIにパターンの一致を教えます。DQN（Deep Q-Network）と呼ばれるアルゴリズムを使用します。名前についてあまり心配する必要はありません - これはAIに決断を下すことを教える一つの方法に過ぎません。

:::note[トレーニングオプション]
<details>
    <summary>トレーニング設定の理解</summary>

    AIの学習方法を設定する必要があります：

    - `batchSize`：一度に学習する経験の数
    複数の過去の試行をまとめて振り返るようなものです。

    - `memorySize`：記憶する過去の経験の数
    何が効果的で何がそうでなかったかのノートを保存するようなものです。

    - `gamma`：将来の報酬をどれだけ重視するか
    値が高い（1に近い）ほど、AIは長期的に考えます。

    - `epsStart`と`epsEnd`：探索と既知の活用のバランス
    新しい戦略を試すか、効果が確認された戦略を使うかのバランスです。

    すべてのパラメータとその効果の詳細な説明については、[パラメータリファレンスガイド](../../reference/hyperparameters)をご覧ください。
</details>
:::

こちらがトレーニングのセットアップ方法です：

```csharp title="Program.cs"
using RLMatrix.Agents.Common;
using RLMatrix;
using PatternMatchingExample;

Console.WriteLine("パターンマッチングトレーニングを開始しています...\n");

// AIの学習方法を設定
var learningSetup = new DQNAgentOptions(
    batchSize: 32,      // 一度に32の経験から学習
    memorySize: 1000,   // 直近1000回の試行を記憶
    gamma: 0.99f,       // 将来の報酬を大切にする
    epsStart: 1f,       // 最初はすべてを試す
    epsEnd: 0.05f,      // 最終的には効果的なものを使用
    epsDecay: 150f      // どれくらい速く移行するか
);

// 環境を作成
var environment = new PatternMatchingEnvironment().RLInit();
var env = new List<IEnvironmentAsync<float[]>> { 
    environment,
    //new PatternMatchingEnvironment().RLInit() //並列トレーニング用にさらに追加可能
};

// 学習エージェントを作成
var agent = new LocalDiscreteRolloutAgent<float[]>(learningSetup, env);

// 学習させましょう！
for (int i = 0; i < 1000; i++)
{
    await agent.Step();

    if ((i + 1) % 50 == 0)
    {
        Console.WriteLine($"ステップ {i + 1}/1000 - 直近50ステップの正確さ: {environment.RecentAccuracy:F1}%");
        environment.ResetStats();
        
        Console.WriteLine("\n続行するにはEnterキーを押してください...");
        Console.ReadLine();
    }
}

Console.WriteLine("\nトレーニングが完了しました！");
Console.ReadLine();
```

このコードを実行すると、50ステップごとにトレーニングの進捗が表示されます：

```bash title="トレーニングの進捗"
パターンマッチングトレーニングを開始しています...

ステップ 50/1000 - 直近50ステップの正確さ: 48.0%
続行するにはEnterキーを押してください...

ステップ 100/1000 - 直近50ステップの正確さ: 68.0%
続行するにはEnterキーを押してください...

ステップ 150/1000 - 直近50ステップの正確さ: 86.0%
続行するにはEnterキーを押してください...

ステップ 200/1000 - 直近50ステップの正確さ: 82.0%
続行するにはEnterキーを押してください...
```

:::tip[何を期待するか]
トレーニングの進捗を観察すると、AIが向上していくのがわかります：

1. 最初は約50%の正確さ（ランダムな推測）
2. 学習するにつれて着実に正確さが向上
3. 最終的には80-90%以上の正確さに到達

50ステップごとの一時停止により、ランダムな推測から熟練したマッチングへの進化を観察できます。これが強化学習の実践です！
:::

## シンプルなマッチングを超えて

私たちの例はシンプルですが、同じ原則がはるかに複雑な問題にも適用されます：

:::note[実世界への応用]
<details>
    <summary>これがどこに繋がるか</summary>

    ここで使用した同じ基本構造は以下のようなものにスケールします：
    - ゲームをプレイするAI
    - ロボット制御
    - リソース管理
    - 交通最適化

    主な違いは状態や行動の複雑さであり、基本的なアプローチではありません。
</details>
:::

## 理解度テスト
<Quiz
    title="強化学習の基本を理解する"
    questions={[
        {
            question: "タスクに対して従来のプログラミングではなく強化学習を選ぶのはなぜですか？",
            options: [
                {
                    text: "プログラムが極めて高い精度で動作する必要がある場合",
                    correct: false,
                    explanation: "実際、正確に何を望んでいるかわかっている場合、従来のプログラミングは精度において優れています。強化学習は、ルールが複雑または未知の場合に真価を発揮します。必ずしも最大の精度が目標ではありません。"
                },
                {
                    text: "ルールが手動でプログラムするには複雑すぎる場合、または自分自身がルールを完全に理解していない場合",
                    correct: true,
                    explanation: "その通りです！強化学習は、ルールが複雑すぎて指定できない場合（ロボットのバランスを取るなど）や、最適なアプローチを私たち自身が完全に理解していない場合に特に価値があります。AIは明示的にプログラムされるのではなく、経験を通じて解決策を発見できます。"
                },
                {
                    text: "プログラムが従来のコードよりも速く実行される必要がある場合",
                    correct: false,
                    explanation: "強化学習は実行速度に関するものではありません - 実際、従来のプログラミングは通常より速く実行されます。強化学習は、あらゆる状況に対して明示的にコードを書く代わりに、経験からプログラムに学習させることに関するものです。"
                }
            ],
            hint: "従来のif/elseプログラミングの限界と、試行錯誤によってパターンを発見させるシステムとの違いを考えてみましょう。"
        },
        {
            question: "例では、epsStartを1.0に、epsEndを0.05のような低い値に設定することが重要だったのはなぜですか？",
            options: [
                {
                    text: "エージェントが常に最高の報酬をもたらす行動を選択することを保証するため",
                    correct: false,
                    explanation: "それは目的ではありません。エージェントが常に最善と思われるもの（搾取のみ）を選んでいたら、まだ試していないより良い戦略を発見することはできないでしょう。"
                },
                {
                    text: "これらの設定は時間の経過とともにエージェントの学習率を制御する",
                    correct: false,
                    explanation: "これらのパラメータは時間とともに変化しますが、学習率（それは'lr'パラメータです）を直接制御するものではありません。強化学習における他の基本的なものを制御します。"
                },
                {
                    text: "これにより、探索（新しいことを試す）と活用（効果があるものを使う）のバランスが時間とともに変化する",
                    correct: true,
                    explanation: "その通りです！これは古典的な探索-活用のバランスです。epsStart: 1fから始めることで、エージェントは最初にすべてを試します（純粋な探索）。トレーニングが進むにつれ、徐々にepsEnd: 0.05fに向かって移行し、学んだ最も効果的なものを主に使用する（主に活用）が、時々は探索も続けます。"
                }
            ],
            hint: "トレーニングの最初と後半でのエージェントの行動がどのように変化するか、そしてなぜそれが重要なのかを考えてみましょう。"
        },
        {
            question: "報酬関数を変更して、正しい一致に対してのみ+1を与え、不正解に対してペナルティを与えない場合、何が起こる可能性が高いですか？",
            options: [
                {
                    text: "エージェントは肯定的なフィードバックのみを受け取るため、学習が速くなる",
                    correct: false,
                    explanation: "ペナルティがなければ、エージェントの学習は実際にはより遅くなるか、まったく学習しない可能性があります。肯定的な報酬のみでは、ランダムな推測でも50%の確率で報酬を得るため、ランダムチャンス以上に改善するインセンティブがほとんどありません。"
                },
                {
                    text: "エージェントが不正確な行動に関する明確なフィードバックを受け取らないため、学習が遅くなるか失敗する",
                    correct: true,
                    explanation: "その通りです！これは適切に設計された報酬関数の重要性を強調しています。不正解に対するペナルティがなければ、エージェントはミスをした時に正誤を区別するフィードバックを受け取りません。ランダムな推測でも半分の時間は報酬を受け取るため、それで十分だと結論付ける可能性があります。"
                },
                {
                    text: "エージェントは同じパターンを学習するが、経験を保存するためにより多くのメモリが必要になる",
                    correct: false,
                    explanation: "メモリ要件は報酬構造に直接関連していません。ここでの重要な問題は、エージェントが受け取る学習シグナルの質であり、使用するメモリ量ではありません。"
                }
            ],
            hint: "学習を動機付けるものは何か - 報酬を受け取るだけなのか、それともペナルティを避けることも含まれるのかを考えてみましょう。"
        },
        {
            question: "gamma（例では0.99fに設定）は学習プロセスでどのような役割を果たしますか？",
            options: [
                {
                    text: "エージェントが一度に記憶できるパターンの数を決定する",
                    correct: false,
                    explanation: "パターンの記憶容量は主にニューラルネットワークのアーキテクチャに関連しており、gammaパラメータではありません。gammaはエージェントが報酬をどのように評価するかという異なる目的に役立ちます。"
                },
                {
                    text: "エージェントが即時の報酬と将来の潜在的な報酬をどれだけ重視するかを制御する",
                    correct: true,
                    explanation: "正解です！gammaは、エージェントが将来の報酬と即時の報酬をどのように比較するかを決定する割引係数です。0.99fという高い設定では、エージェントは即時の報酬とほぼ同じくらい将来の報酬を気にかけ、長期的に良い結果をもたらす戦略を学習するよう促します。"
                },
                {
                    text: "エージェントが失敗した試みをどれだけ早く忘れるかを設定する",
                    correct: false,
                    explanation: "エージェントの過去の経験の記憶はmemorySizeパラメータによって制御され、gammaではありません。gammaはエージェントが時間の経過とともに行動の価値をどのように評価するかに影響します。"
                }
            ],
            hint: "より複雑な環境では、行動が常に即時の報酬につながるわけではありません。エージェントがどのように今の小さな報酬と後の潜在的に大きな報酬の間で決定するかを考えてみましょう。"
        },
        {
            question: "学んだことに基づいて、次のタスクのうちどれが強化学習アプローチに最も適していますか？",
            options: [
                {
                    text: "数字のリストを昇順にソートする",
                    correct: false,
                    explanation: "ソートは最適なアルゴリズムが既に知られている、よく理解された問題です。任意の入力に対して正確な出力が何であるべきかを正確に知っているため、従来のプログラミングがより適切でしょう。"
                },
                {
                    text: "複雑な関節力学を持つシミュレーションロボットのバランスを取る",
                    correct: true,
                    explanation: "完璧な選択です！ロボットのバランス取りには正確にモデル化するのが難しい複雑な物理が関わり、バランスを維持するための多くの潜在的な戦略があります。これは強化学習が輝く場面の典型例です - ルールが複雑で最適な方針が人間にとっても明白でない場合です。"
                },
                {
                    text: "摂氏と華氏の間で温度を変換する",
                    correct: false,
                    explanation: "これは従来のプログラミングで簡単に実装できる単純な数学的公式（F = C × 9/5 + 32）です。各入力に対して単一の正解があり、このタスクに対して強化学習は不必要に複雑になります。"
                }
            ],
            hint: "明示的に指定するのが難しいが、試行錯誤を通じて学習できるルールを持つタスクはどれかを考えてみましょう。"
        }
    ]}
/>

## 次のステップ

さらに進みたいですか？次のステップとして：
- [ダッシュボードの使い方](../beginner/howtodashboard)
- [連続的なアクションの扱い方](../beginner/continuousactions)
- [カートポールの例](../beginner/cartpole)

利用可能な2つの主要なアルゴリズムがあります：
- DQN：今使用したもので、シンプルな選択に適しており、大きなリプレイメモリから恩恵を受けます。
- PPO：より高度で、連続的なアクション（速度や方向の制御など）を扱えます。

:::note[進捗の理解]
<details>
    <summary>重要なポイント</summary>

    学んだこと：
    1. 強化学習が従来のプログラミングとどのように異なるか
    2. 基本的な学習環境を作成する方法
    3. トレーニングをセットアップして実行する方法
    4. より複雑なアプリケーションのための基本要素

    最も重要なのは、明示的な指示ではなく経験を通じてコンピュータに教えることができることを見たことです。
</details>
:::