---
title: 連続的なアクションの扱い方
description: RLMatrixと強化学習における連続的アクションの紹介
---

import { LinkCard } from '@astrojs/starlight/components';
import Quiz from '@/components/Quiz.astro';

[前回のチュートリアル](/beginner/gettingstarted/)のプロジェクトから始めて、連続的アクションを追加してみましょう。[スタータープロジェクト](https://github.com/asieradzk/RLMatrixGettingStartedExample1)を使って一緒に進めるか、[完成したプロジェクト](https://github.com/asieradzk/RLMatrixGettingStartedExample2_ContinuousActions)を確認してもよいでしょう。

## 離散的アクションと連続的アクション

前回のガイドでは、離散的アクションを扱いました - エージェントはパターンに一致するために有限の選択肢（0または1）から選ぶ必要がありました。実際のシナリオでは、どのボタンを押すかを決定するために多数のセンサーデータや視覚入力を受け取ることがあります。

:::tip[プロジェクトを将来に備える]
もし可能であれば、アクション空間を離散化して決定を有限個のボタン押し（離散的アクション）に単純化しましょう！これにより学習信号がはるかに見えやすくなり、このチュートリアルで実際に体験するように、トレーニングが劇的に加速します。
:::

しかし、多くの実世界のアプリケーションでは、これが常に可能とは限りません。例えば以下のような制御には：
- 車両のステアリング角度
- ロボットアームの関節トルク
- エンジンの出力レベル

エージェントは連続的アクション、つまりカテゴリ的な選択ではなく、正確な浮動小数点値を出力する必要があります。

## 環境に連続的アクションを追加する

環境を修正して、離散的アクションと連続的アクションの両方を含めましょう。元のパターンマッチングタスクを維持しつつ、AIが新しい値の平方根を出力すると期待する第2のパターンを追加します。

私たちの「期待」以外は何も変えないことに注目してください - エージェントは報酬シグナルだけを頼りに、試行錯誤を通じて私たちが望むことを理解する必要があります！

まず、`PatternMatchingEnvironment.cs`に第2のパターンと連続的アクションを追跡するための新しいフィールドを追加します：

```csharp title="PatternMatchingEnvironment.cs" ins="private int pattern2 = 0;" ins="private float aicontinuousChoice = 0f;"
private int pattern = 0;
private int pattern2 = 0;
private int aiChoice = 0;
private float aicontinuousChoice = 0f;
private bool roundFinished = false;
```

次に、2つ目の観測メソッドと連続的アクションメソッドを追加します：

```csharp title="PatternMatchingEnvironment.cs"
[RLMatrixObservation]
public float SeePattern() => pattern;

[RLMatrixObservation]
public float SeePattern2() => pattern2;

[RLMatrixActionContinuous]
public void MakeChoiceContinuous(float input)
{
    aicontinuousChoice = input;
}
```

次に、報酬関数を作成しましょう：

```csharp title="PatternMatchingEnvironment.cs"
[RLMatrixReward]
public float GiveReward() => aiChoice == pattern ? 1.0f : -1.0f;

// AIの連続的な出力が第2パターンの平方根に近い場合に+2の報酬を追加
[RLMatrixReward]
public float ExtraRewards() => Math.Abs(aicontinuousChoice - Math.Sqrt(pattern2)) < 0.1f ? 2f : 0.0f;
```

最後に、両方のパターンを生成するように`StartNewRound`メソッドを更新する必要があります：

```csharp title="PatternMatchingEnvironment.cs" ins="pattern2 = Random.Shared.Next(10);"
[RLMatrixReset]
public void StartNewRound()
{
    pattern = Random.Shared.Next(2);
    pattern2 = Random.Shared.Next(10);
    aiChoice = 0;
    roundFinished = false;
}
```

pattern2には0-9の範囲を使用していることに注目してください。これによりエージェントにさまざまな平方根を予測するという、より興味深い課題を与えます。

## コンパイルエラーの修正

ソリューションをビルドしようとすると、一連のエラーが発生します。これは実際には役立つことです - RLMatrixは実行時エラーを防ぎ、連続的アクションの正しい実装に導くために強力な型付けを使用しています。

### エラー1：環境の型の不一致

```
Argument 1: cannot convert from 'PatternMatchingExample.PatternMatchingEnvironment' to 'RLMatrix.IEnvironmentAsync<float[]>'
```

これは、RLMatrixが型の安全性を確保するために連続的環境と離散的環境に異なるインターフェースを持っているために発生します。`Program.cs`のコードを更新しましょう：

```diff lang="csharp" title="Program.cs - 環境の型"
-var env = new List<IEnvironmentAsync<float[]>> {
+var env = new List<IContinuousEnvironmentAsync<float[]>> {
    environment,
    //new PatternMatchingEnvironment().RLInit() //並列トレーニング用にさらに追加可能
};
```

### エラー2：エージェントの型の不一致

この変更の後、2つ目のエラーが発生します：

```
Argument 2: cannot convert from 'System.Collections.Generic.List<RLMatrix.IContinuousEnvironmentAsync<float[]>>' to 'System.Collections.Generic.IEnumerable<RLMatrix.IEnvironmentAsync<float[]>>'
```

これは、連続的環境に離散的エージェントを使用しようとしているためです。エージェントの型を変更する必要があります：

```diff lang="csharp" title="Program.cs - エージェントの型"
-var agent = new LocalDiscreteRolloutAgent<float[]>(learningSetup, env);
+var agent = new LocalContinuousRolloutAgent<float[]>(learningSetup, env);
```

### エラー3：アルゴリズムオプションの不一致

これにより3つ目のエラーが発生します：

```
Argument 1: cannot convert from 'RLMatrix.DQNAgentOptions' to 'RLMatrix.PPOAgentOptions'
```

この最後のエラーは、DQNが連続的アクションと互換性がないことを示しています。離散的アクション空間と連続的アクション空間の両方を扱えるPPO（近位方策最適化）に切り替える必要があります：

```diff lang="csharp" title="Program.cs - アルゴリズムオプション"
-var learningSetup = new DQNAgentOptions(
-    batchSize: 32,      
-    memorySize: 1000,   
-    gamma: 0.99f,      
-    epsStart: 1f,     
-    epsEnd: 0.05f,      
-    epsDecay: 150f      
-);
+var learningSetup = new PPOAgentOptions(
+    batchSize: 128,
+    memorySize: 1000,
+    gamma: 0.99f,
+    width: 128,
+    lr: 1E-03f
+);
```

:::note[DQN対PPO]
DQNは特に離散的アクション空間向けに設計されており、連続的な出力を扱うメカニズムがありません。一方、PPOはアクター・クリティックアルゴリズムであり、離散的アクション、連続的アクション、またはその両方を同時に扱うことができます。

それぞれのアルゴリズムには強みがあります - DQNは離散的な問題にはサンプル効率が非常に高く、PPOは複雑な環境をより堅牢に扱うことができます。RLMatrixは両方を提供しているので、特定のニーズに基づいて選択できます。
:::

## 最初のトレーニング実行

では、トレーニングを実行して何が起こるか見てみましょう：

```bash title="トレーニング出力"
Step 800/1000 - Last 50 steps accuracy: 42.0%
Press Enter to continue...

Step 850/1000 - Last 50 steps accuracy: 38.0%
Press Enter to continue...

Step 900/1000 - Last 50 steps accuracy: 40.0%
Press Enter to continue...

Step 950/1000 - Last 50 steps accuracy: 38.0%
Press Enter to continue...

Step 1000/1000 - Last 50 steps accuracy: 37.0%
Press Enter to continue...
```

驚きです！AIはほとんど学習していません。正確さは50%を超えず、ダッシュボードを確認すると、離散的アクション（パターンの一致）に対して+1の報酬を定期的に収集していますが、連続的アクション（√pattern2の予測）に対する+2の報酬はほとんど得られていません。

## なぜこれが起こるのか？

自問してみてください：なぜAIは連続的アクションよりも離散的アクションの方が簡単に学習できるのでしょうか？

最初の直感は学習率（`lr`）かもしれません - 低すぎるのでしょうか？`1E-02f`に変更してトレーニングを再実行してみましょう...

効果はありましたか？おそらくないでしょう。実際、エージェントは離散的アクションをより速く学習する一方で、連続的アクション空間をほとんど探索せず、トレーニングが進むにつれて正確さはさらに悪化することに気づくかもしれません。

では実際には何が起きているのでしょうか？

:::caution[根本的な課題]
AIがランダムな探索を通じて連続的アクションを正確に当てる可能性は極めて低いです。

考えてみてください：
- 離散的な選択（0または1）では、ランダムな推測でも正解である確率は50%です
- 連続的な値では、ランダムに以下を出力する確率はどれくらいでしょうか：
- pattern2 = 0の時に√0 = 0
- pattern2 = 1の時に√1 = 1
- pattern2 = 2の時に√2 ≈ 1.414
- pattern2 = 3の時に√3 ≈ 1.732
- ...そして√9 = 3まで

これは疎な報酬問題を生み出します - エージェントが偶然に正しいアクションを見つける可能性は非常に低いため、学習するための有用なフィードバックをほとんど受け取れません。
:::

## 誘導シグナルの追加

より役立つ報酬シグナルを提供することでこれを解決してみましょう。正確な一致だけに報酬を与えるのではなく、エージェントが正しい平方根に近づくにつれて増加する報酬を追加します：

```csharp title="PatternMatchingEnvironment.cs"
[RLMatrixReward]
public float ExtraSupportingReward() => 0.5f / (1 + Math.Abs(aicontinuousChoice - (float)Math.Sqrt(pattern2)));

//学習率を1E-03fに戻すことを忘れないでください！
```

この報酬関数は勾配を作成します - エージェントが正しい値に近づくにつれて強くなる連続的なシグナルです。完全に正確でなくても、「温かい」か「冷たい」かについてのフィードバックを得られます。

:::tip[報酬エンジニアリング]
これは強化学習における重要な原則である**報酬設計**を示しています：

- **疎な報酬**（全か無かの報酬）は連続的な空間での学習をほぼ不可能にします
- **密な/形成された報酬**は学習プロセスを導く勾配を作成します
- 「温かくなっている」という小さなシグナルでさえ、数時間で学習できるか全く学習できないかの違いを生み出す可能性があります

これは目隠しされた人が部屋を横切る手助けをするようなものです：
- 疎な報酬：「ドアに到達しました」（それ以外は沈黙）
- 形成された報酬：「温かい...温かい...冷たい...また温かい...」

2つ目のアプローチは、より確実に成功に導きます。これは特に、エージェントが正確な値を発見する必要がある連続的制御問題において極めて重要です。
:::

この変更でトレーニングを再実行し、何が起こるか見てみましょう：

```bash title="トレーニング出力"
Step 850/1000 - Last 50 steps accuracy: 35.0%
Press Enter to continue...

Step 900/1000 - Last 50 steps accuracy: 40.0%
Press Enter to continue...

Step 950/1000 - Last 50 steps accuracy: 47.0%
Press Enter to continue...

Step 1000/1000 - Last 50 steps accuracy: 36.0%
Press Enter to continue...
```

若干の改善が見られますが、まだ十分ではありません。ダッシュボードには学習が進んでいるヒントが表示されるかもしれませんが、明らかにこのより複雑なタスクにはより多くのトレーニング時間が必要です。

## トレーニング時間の延長

連続的アクション予測のようなより複雑な課題では、より多くのトレーニングステップが必要になることがよくあります。プログラムを修正して1,000ステップではなく10,000ステップでトレーニングするようにしましょう：

```csharp title="Program.cs" {1,5}
for (int i = 0; i < 10000; i++)
{
    await agent.Step();

    if ((i + 1) % 500 == 0)
    {
        Console.WriteLine($"Step {i + 1}/10000 - Last 500 steps accuracy: {environment.RecentAccuracy:F1}%");
        environment.ResetStats();
        
        Console.WriteLine("\nPress Enter to continue...");
        Console.ReadLine();
    }
}
```

## 実験：学習率の影響

より長いトレーニングの進行を観察しながら、異なる学習率を試してみてください。さらに低くするとどうなりますか？大幅に上げるとどうなりますか？

私の実験では、非常に高い学習率を設定すると、モデルは離散的アクションに対する+1の報酬のみを収集することに固執し、連続的な空間を適切に探索することに完全に失敗します。

:::tip[学習率についての洞察]
特にPPOでは、学習率を上げることが常に探索に良いとは限りません：

- **高すぎる**：エージェントは最初に見つけた戦略にすぐに固執し、発見が難しい連続的アクションパターンを無視することがよくあります
- **低すぎる**：学習は非常に遅く進みますが、より徹底的に探索します
- **ちょうど良い**：タスクに適した探索と活用のバランスを取ります

この直感に反する学習率と探索の質の関係は、連続的アクション空間を扱う際に特に重要です。
:::

## 重要なポイント

この演習を通じて、いくつかの重要な教訓を学びました：

1. **連続的アクションは疎な報酬問題のため、離散的アクションよりも本質的に学習が難しい**です。可能であれば、アクション空間を離散化しましょう！

2. **報酬エンジニアリングは連続的制御問題において非常に重要**です。「温かくなっている」というシグナルを提供することで、不可能な学習タスクを扱いやすいものに変えることができます。

3. **複雑なタスクにはより多くのトレーニング時間が必要**です。アクション空間に次元を追加するにつれて、トレーニング期間も適切にスケールする必要があります。

4. **アルゴリズムの選択は重要**です。DQNは連続的アクションをまったく扱えませんが、PPOは離散的、連続的、または混合アクション空間を扱うことができます。

5. **学習率の調整は繊細**です、特にPPOでは。高ければ常に良いというわけではなく、探索に対してはむしろ悪影響を及ぼすこともあります。

これらの原則は、RLMatrixでより複雑な強化学習の課題に取り組む際に役立つでしょう。

## 理解度テスト

<Quiz
    title="連続的アクションの理解"
    questions={[
        {
            question: "エージェントが離散的アクションと比較して連続的アクションを学習するのがはるかに難しい理由は何ですか？",
            options: [
                {
                    text: "連続的アクションはニューラルネットワーク計算の複雑さにより、より多くの計算リソースを必要とするため",
                    correct: false,
                    explanation: "連続的アクション空間はより複雑なニューラルネットワークを必要とするかもしれませんが、これが学習の難しさの根本的な理由ではありません。核心的な課題は強化学習における探索-活用問題にはるかに根本的なものです。"
                },
                {
                    text: "正しい連続的値をランダムに探索する確率は、少数の離散的オプションから選ぶことと比較して無限に小さいため",
                    correct: true,
                    explanation: "その通りです！これは疎な報酬問題の現れです。ランダムに探索する場合、エージェントは離散的な二項選択を正しく得る確率は約50%ですが、すべての可能な浮動小数点値から正確に√2 ≈ 1.414...を見つけることは純粋な偶然ではほぼ不可能です。これにより、適切な報酬設計がなければ初期の学習シグナルは非常に稀になります。"
                },
                {
                    text: "PPOアルゴリズムはあらゆる種類の学習タスクにおいてDQNアルゴリズムよりも本質的に効率が悪いため",
                    correct: false,
                    explanation: "これは正確ではありません。PPOとDQNにはそれぞれ強みがあります - DQNは離散的問題に対してサンプル効率が高く、PPOはより汎用的であり、DQNが本質的に扱えない連続的アクション空間を扱うことができます。どちらも普遍的に優れているわけではありません。"
                }
            ],
            hint: "エージェントがトレーニングの最初にランダムに探索している時に何が起こるかを考えてみてください。それぞれの場合で正しいアクションを見つける確率はどれくらいでしょうか？"
        },
        {
            question: "どのような主要な技術が私たちの連続的アクション学習問題をほぼ不可能なものから扱いやすいものに変えましたか？",
            options: [
                {
                    text: "トレーニングステップ数を1,000から10,000に増やすこと",
                    correct: false,
                    explanation: "より多くのトレーニング時間は重要ですが、それだけでは根本的な疎な報酬問題を解決できません。我々のエージェントは、より重要な変更がなければ学習に苦労し続けるでしょう。"
                },
                {
                    text: "DQNからPPOアルゴリズムに変更すること",
                    correct: false,
                    explanation: "この変更は必要でした（DQNは連続的アクションをまったく扱えないため）が、十分ではありませんでした。PPOでさえ、初期段階では疎な報酬シグナルに苦労していました。"
                },
                {
                    text: "エージェントが正しい値にどれだけ近いかに基づいてフィードバックを提供する形成された報酬関数を追加すること",
                    correct: true,
                    explanation: "その通りです！これは報酬設計の実践です。ExtraSupportingReward関数を追加して0.5f / (1 + Math.Abs(aicontinuousChoice - Math.Sqrt(pattern2)))を返すことで、エージェントが完全に正しくなくても有用な学習シグナルを提供する勾配を作成しました。これは、正確なターゲットを見つけるまで沈黙するのではなく、「温かい/冷たい」フィードバックを与えるようなものです。"
                }
            ],
            hint: "連続的アクション空間における疎な報酬の根本的な課題に対処したのはどの変更かを考えてみてください。"
        },
        {
            question: "連続的アクションタスクにおけるPPOの学習率について、どのような直感に反する関係を観察しましたか？",
            options: [
                {
                    text: "高い学習率はエージェントが連続的アクションを無視しながら、より容易な離散的報酬にのみ集中する原因となった",
                    correct: true,
                    explanation: "その通りです！非常に高い学習率では、エージェントは離散的アクションパターン（+1の報酬）をすぐに学習し、それに固執する傾向があることを観察しました。これにより、+2の報酬を提供するものの発見が難しい連続的アクション空間を効果的に無視することになります。これは学習率が探索/活用のバランスにどのように影響するかを示しています。"
                },
                {
                    text: "低い学習率は常に最適な方針への収束が速くなった",
                    correct: false,
                    explanation: "これは我々の観察とは逆です。低い学習率は実際には全体的な学習が遅くなりましたが、連続的アクション空間のより良い探索につながることもありました。微妙なバランスを取ることが必要です。"
                },
                {
                    text: "学習率はトレーニング結果に有意義な影響を与えなかった",
                    correct: false,
                    explanation: "学習率はトレーニング結果に大きな影響を与えました。学習の速度だけでなく、より重要なことに、特にエージェントがより容易な離散的報酬と発見が難しい連続的アクションパターンのどちらを優先するかという点で、探索-活用のバランスに影響しました。"
                }
            ],
            hint: "異なる学習率を実験した時に何が起きたか、そしてそれがエージェントの両方のタイプの報酬を発見する能力にどのように影響したかを思い出してください。"
        },
        {
            question: "4つのボタンからどのボタンを押すかを決定し、さらにどれだけの圧力（0-100%）をかけるかを決定するロボット制御システムを設計する場合、このレッスンに基づいて最も効果的なアプローチは何ですか？",
            options: [
                {
                    text: "効率性のためDQNを使用し、圧力を10段階（0%、10%、20%など）に離散化する",
                    correct: true,
                    explanation: "素晴らしい選択です！これはチュートリアルの重要な原則に従っています：「アクション空間を離散化できるなら、それを行いましょう！」圧力を10段階の離散値に変換することで、難しい連続的問題を合計40のアクション（4つのボタン×10の圧力レベル）だけの扱いやすい離散的な問題に変換しました。DQNはこの離散化された空間を、連続値と格闘するよりもはるかに効率的に学習します。"
                },
                {
                    text: "デフォルト設定でPPOを使用し、両方の側面を同時に理解させる",
                    correct: false,
                    explanation: "PPOは混合アクション空間を扱うことができますが、デフォルト設定だけで使用すると最適ではない学習につながる可能性があります。このレッスンでは、連続的アクションはランダムな探索を通じて本質的に学習が難しいことを示しました。離散化または慎重な報酬設計がなければ、学習は非効率的になるでしょう。"
                },
                {
                    text: "圧力の精度に関する勾配フィードバックを提供する形成された報酬を持つPPOを使用し、圧力を真の連続値として扱う",
                    correct: false,
                    explanation: "このアプローチは最終的には機能する可能性がありますが、可能な場合にアクションを離散化することがより速く、より信頼性の高い学習につながるというチュートリアルの重要な洞察を無視しています。圧力を連続値として扱うことは、10段階の離散化でタスクに十分な場合に、不必要に難しい学習問題を作り出します。"
                }
            ],
            hint: "可能な場合にアクション空間を離散化することに関するチュートリアルの強い推奨を思い出し、どのアプローチが学習問題を最も単純化するかを考えてください。"
        },
        {
            question: "これまでに見たパターンに基づいて、強化学習エージェントにとって最も難しいと思われるシナリオはどれですか？",
            options: [
                {
                    text: "迷路の3つの既定ルートのうちの1つを選ぶことを学習する",
                    correct: false,
                    explanation: "これは3つのオプションだけの簡単な離散的アクション問題です。このレッスンによれば、少数のオプションを持つ離散的アクション空間は強化学習エージェントが効果的に探索するのに最も簡単です。"
                },
                {
                    text: "システムの温度を正確に37.85°Cに最小限の変動で調整することを学習する",
                    correct: true,
                    explanation: "これは確かに最も難しいシナリオです！非常に正確な連続値（正確に37.85°C）を見つけ、最小限の偏差でそれを維持することが必要です。適切に形成された報酬がなければ、エージェントはランダムな探索を通じてこの正確な設定値を発見するのに苦労するでしょう。まさに私たちが議論した疎な報酬問題の一種です。"
                },
                {
                    text: "視覚入力に基づいて10種類の異なるオブジェクトを認識して分類することを学習する",
                    correct: false,
                    explanation: "これはより多くの離散的オプション（10カテゴリ）を含みますが、基本的には依然として離散的分類問題です。エージェントは分類が正しいかどうかについて明確なフィードバックを受け取り、連続的アクション空間の疎な報酬という課題を回避できます。"
                }
            ],
            hint: "アクション空間の探索という点で、針を干し草の山から見つけるようなシナリオはどれかを考えてみてください。"
        }
    ]}
/>

## 次のステップ

連続的アクション空間の課題とその対処法を理解したので、より複雑な観測を伴う古典的な強化学習問題に挑戦する準備が整いました。

<LinkCard
    title="カートポールの例"
    description="強化学習を使用して移動するカート上のポールのバランスを取ることを学びます。"
    href="/beginner/cartpole/"
/>