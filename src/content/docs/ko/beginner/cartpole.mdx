---
title: 카트폴 예제
description: 강화학습을 통해 움직이는 카트 위에서 막대의 균형을 유지하는 방법 배우기
---

import Quiz from '@/components/Quiz.astro';

import { LinkCard } from '@astrojs/starlight/components';

강화학습을 실제로 볼 준비가 되셨나요? 이 튜토리얼에서는 AI가 움직이는 카트 위에서 막대를 수직으로 유지하는 방법을 배우는 것을 지켜볼 수 있는 고전적인 균형 유지 과제에 도전해 보겠습니다.

<video autoplay loop muted playsinline width="100%" style="cursor: pointer;" onclick="this.paused ? this.play() : this.pause();">
    <source src="/images/cartpole/cartpole.mp4" type="video/mp4" />
    <source src="/images/cartpole/cartpole.webm" type="video/webm" />
    브라우저가 HTML5 비디오를 지원하지 않습니다.
</video>

카트폴 과제는 단순함과 시각적 피드백을 결합하여 강화학습에 완벽한 환경을 제공합니다. 카트를 좌우로 밀면 물리 법칙에 따라 부착된 막대가 균형을 유지하거나 넘어집니다. 매 시간 단계마다 에이전트가 결정을 내리고, 알고리즘이 점차적으로 과제를 마스터해 나가는 모습을 지켜보는 만족감을 얻을 수 있습니다.

:::note[기본을 넘어서]
<details>
    <summary>이 과제가 특별한 이유는 무엇인가요?</summary>

    카트폴은 너무 사소하지도, 너무 압도적이지도 않은 완벽한 지점에 위치하기 때문에 벤치마크로 오랫동안 사용되어 왔습니다:

    - 에이전트의 개선을 실시간으로 관찰할 수 있습니다
    - 물리 법칙은 직관적이지만 제어를 마스터하는 것은 쉽지 않습니다
    - 훈련이 빠르게 완료됩니다(몇 시간이 아닌 몇 분)
    - 성공 여부가 명확합니다 - 막대가 올라가 있거나 그렇지 않거나
    - 이 기술은 더 복잡한 제어 문제에 직접 적용할 수 있습니다

</details>
:::

## 프로젝트 설정하기

시뮬레이션된 물리 환경을 제공하기 위해 [SciSharp/Gym.NET](https://github.com/SciSharp/Gym.NET)을 사용할 것입니다.

:::caution[Windows 필수]
이 예제는 렌더링을 위해 Windows Forms에 의존합니다. 다른 운영 체제를 사용하는 경우 Gym.NET용 Avalonia 기반 렌더러가 있지만 여기서는 다루지 않을 것입니다. 대안으로 여러 플랫폼에서 작동하는 예정된 Godot 기반 예제를 확인해 보세요.
:::

따라하거나 원하는 경우 [완성된 프로젝트](https://github.com/asieradzk/RLMatrixGettingStartedExample3_CartPole)를 다운로드할 수 있습니다.

필요한 패키지를 설치해 봅시다:

```bash title="필요한 패키지 설치하기"
dotnet add package RLMatrix
dotnet add package RLMatrix.Toolkit
dotnet add package Gym.NET
dotnet add package Gym.NET.Environments
dotnet add package Gym.NET.Rendering.WinForm
```

## 환경 구축하기

다음은 카트폴 환경 구현입니다:

```csharp title="CartPoleEnvironment.cs"
using System;
using System.Threading.Tasks;
using Gym.Environments.Envs.Classic;
using Gym.Rendering.WinForm;
using RLMatrix.Toolkit;
using NumSharp;

namespace MyEnv
{
    [RLMatrixEnvironment]
    public partial class CartPoleEnvironment
    {
        private CartPoleEnv myEnv;
        private float[] myState;
        private int stepCounter;
        private const int MaxSteps = 100000;
        private bool isDone;

        public CartPoleEnvironment()
        {
            InitialiseAsync();
        }

        private void InitialiseAsync()
        {
            myEnv = new CartPoleEnv(WinFormEnvViewer.Factory);
            ResetEnvironment();
        }

        [RLMatrixObservation]
        public float GetCartPosition() => myState[0];

        [RLMatrixObservation]
        public float GetCartVelocity() => myState[1];

        [RLMatrixObservation]
        public float GetPoleAngle() => myState[2];

        [RLMatrixObservation]
        public float GetPoleAngularVelocity() => myState[3];

        [RLMatrixActionDiscrete(2)]
        public void ApplyForce(int action)
        {
            if (isDone)
                ResetEnvironment();

            var (observation, reward, done, _) = myEnv.Step(action);
            myEnv.Render();
            myState = ToFloatArray(observation);
            isDone = done;
            stepCounter++;

            if (stepCounter > MaxSteps)
                isDone = true;
        }

        private float[] ToFloatArray(NDArray npArray)
        {
            double[] doubleArray = npArray.ToArray<double>();
            return Array.ConvertAll(doubleArray, item => (float)item);
        }

        [RLMatrixReward]
        public float CalculateReward()
        {
            return isDone ? 0 : 1;
        }

        [RLMatrixDone]
        public bool IsEpisodeFinished()
        {
            return isDone;
        }

        [RLMatrixReset]
        public void ResetEnvironment()
        {
            myEnv.Reset();
            myState = new float[4] { 0, 0, 0, 0 };
            isDone = false;
            stepCounter = 0;
        }
    }
}
```

:::tip[여기서 무슨 일이 일어나고 있나요?]
우리의 환경은 에이전트에 네 가지 정보를 제공합니다:
- 카트의 위치와 속도
- 막대의 각도와 회전 속도

에이전트는 두 가지 가능한 움직임을 가집니다: 왼쪽으로 밀기(0) 또는 오른쪽으로 밀기(1).

에이전트가 밀 때마다 물리 시뮬레이션을 진행하고, 창을 다시 그리고, 에피소드를 계속해야 하는지 결정합니다. 우리는 막대가 균형을 유지하는 각 시간 단계마다 +1의 보상을 제공하여 더 오래 균형을 유지하도록 장려합니다.

튜플 언패킹 패턴(`var (observation, reward, done, _) = myEnv.Step(action)`)은 Gym.NET의 Python 기원에서 상속되었습니다. 이것은 작동하지만 RLMatrix의 설계 철학과 완벽하게 일치하지는 않습니다.
:::

## 훈련 설정하기

이제 에이전트에게 균형을 가르칠 훈련 코드를 살펴보겠습니다:

```csharp title="Program.cs"
using RLMatrix.Agents.Common;
using RLMatrix;
using MyEnv;

Console.WriteLine("Starting cart-pole training...\n");

// 학습 매개변수 구성
var learningSetup = new PPOAgentOptions(
    batchSize: 8,
    ppoEpochs: 8,
    memorySize: 1000,
    gamma: 0.99f,
    width: 128,
    entropyCoefficient: 0.01f,
    lr: 1E-02f
);

// 환경 생성 및 에이전트에 연결
var environment = new CartPoleEnvironment().RLInit(maxStepsSoft: 1200, maxStepsHard: 1200);
var env = new List<IEnvironmentAsync<float[]>> {
    environment,
    //new CartPoleEnvironment().RLInit() //여러 환경으로 훈련하려면 주석 해제
};

// 에이전트 초기화
var agent = new LocalDiscreteRolloutAgent<float[]>(learningSetup, env);

// 수렴할 때까지 훈련
for (int i = 0; i < 100000; i++)
{
    await agent.Step();
}

Console.WriteLine("\nTraining complete!");
Console.ReadLine();
```

시간 단계당 +1이라는 단순한 보상은 놀라울 정도로 강력합니다. 딥 강화학습 알고리즘은 자연스럽게 장기적인 목표를 최적화하여 미묘하고 선제적인 조정이 더 오래 균형을 유지하고 더 높은 누적 보상으로 이어진다는 것을 알아냅니다.

## RLMatrix의 PPO: 무엇이 다른가요?

:::caution[구현 차이점]
RLMatrix의 PPO 구현은 분산 훈련을 위해 최적화되어 있어, 연구 논문이나 다른 프레임워크에서 볼 수 있는 것과 몇 가지 차이점이 있습니다:

<details>
    <summary>구현을 비교할 때 알아두면 좋은 점</summary>

    1. **배치 크기 해석**: RLMatrix에서 `batchSize`는 모델 업데이트 전에 수집할 완전한 *에피소드*의 수를 의미합니다 - 다른 많은 구현에서처럼 개별 단계 수가 아닙니다.

    2. **온-폴리시 일관성**: PPO는 현재 정책 하에서 수집된 경험으로부터만 학습합니다. 업데이트하기 전에 여러 완전한 에피소드를 수집하는 것은 안정적인 그래디언트 추정을 생성하고, 에피소드 중간에 정책을 업데이트하면 발생할 수 있는 오프-폴리시 오류를 도입하지 않으면서 더 많은 환경 역학을 포착할 수 있게 해줍니다.

    3. **다중 훈련 패스**: `ppoEpochs` 매개변수는 수집된 경험을 통해 몇 번의 패스를 수행할지 제어합니다. 나중에 데이터를 폐기할 것이기 때문에, 여러 패스를 통해 최대한의 가치를 추출하고자 합니다.

</details>
:::

초기 튜토리얼에서 다룬 DQN은 간단한 작업에서 더 샘플 효율적일 수 있지만, PPO는 일반적으로 광범위한 하이퍼파라미터 튜닝 없이도 더 안정적인 훈련을 제공합니다. 이것이 특히 도전적인 제어 문제에 적합한 이유입니다.

## 알아야 할 메모리 절약 기법

훈련 코드의 다음 줄을 살펴보세요:

```csharp
var environment = new CartPoleEnvironment().RLInit(maxStepsSoft: 1200, maxStepsHard: 1200);
```

이 얼핏 보기에는 평범한 매개변수 구성이 GPU 메모리를 압도하지 않고 매우 긴 에피소드로 훈련할 수 있는 비결을 담고 있습니다. 설명해 드리겠습니다:

:::danger[긴 에피소드를 위한 메모리 혁신]
소프트와 하드 에피소드 제한 사이의 구분은 긴 훈련 에피소드를 처리하는 방식을 혁신합니다:

- **maxStepsHard**: 환경 리셋을 강제하기 전의 절대적인 상한선
- **maxStepsSoft**: 물리 시뮬레이션은 계속 진행하면서 보상과 그래디언트 계산을 중단하는 시점

이 메커니즘은 에피소드가 수천 단계 동안 실행될 수 있을 때 매우 유용해집니다.
:::

이 값을 수정하면 어떻게 되나요?

```csharp
var environment = new CartPoleEnvironment().RLInit(maxStepsSoft: 200, maxStepsHard: 1200);
```

이제 마법이 일어납니다:
1. 처음 200단계에 대해서만 보상을 누적하고 그래디언트를 계산합니다
2. 시뮬레이션은 1200단계까지 또는 실패할 때까지 자연스럽게 계속됩니다
3. GPU 메모리 사용량이 크게 감소합니다

이 구성을 실행할 때 보상 그래프를 확인해 보세요 - 카트폴 물리 시뮬레이션이 그 지점 이후에도 계속되지만, 보상이 200(소프트 제한)을 초과하지 않는 것을 알 수 있습니다. 작업 관리자를 열고 실시간으로 메모리 절약을 확인해 보세요.

이 기술은 에피소드가 무기한 실행될 수 있는 복잡한 환경에서 필수적입니다. 메모리 부족 오류로 충돌하는 대신, 자연스러운 환경 역학을 유지하면서 얼마나 많은 계산 노력을 투자할지 정확하게 제어할 수 있습니다.

## 학습 과정 관찰하기

이 훈련을 실행하면 카트폴 환경을 보여주는 창이 나타납니다. 처음에는 막대가 빠르게 넘어질 것입니다 - 에이전트는 무엇을 해야 할지 전혀 모릅니다. 그러나 몇 분 내에 놀라운 변화를 목격하게 될 것입니다:

1. 처음에는 에이전트가 전략 없이 무작위 움직임을 보입니다
2. 그 다음 막대가 이미 떨어지고 있을 때 반응하기 시작합니다(너무 늦음!)
3. 점차적으로 수정 동작을 더 일찍, 더 일찍 수행하는 법을 배웁니다
4. 마지막으로, 미묘하고 선제적인 조정을 통해 막대를 완벽하게 균형 있게 유지합니다

이러한 눈에 보이는 진전이 카트폴을 학습 예제로서 만족스럽게 만듭니다. 그래프에서 숫자가 개선되는 것을 보는 것이 아니라, AI가 당신의 눈앞에서 기술을 발전시키는 것을 지켜보고 있는 것입니다.

## 이해도 테스트하기

<Quiz
    title="카트폴 강화학습 이해하기"
    questions={[
        {
            question: "카트폴이 이상적인 강화학습 예제로 간주되는 이유는 무엇인가요?",
            options: [
                {
                    text: "다른 RL 문제에 비해 최소한의 계산 리소스만 필요하기 때문에",
                    correct: false,
                    explanation: "카트폴이 일부 복잡한 환경보다 리소스 집약적이지 않지만, 튜토리얼은 학습 예제로서의 가치에 대한 다른 이유를 강조합니다. 계산 효율성은 주요 장점이 아닙니다."
                },
                {
                    text: "에이전트의 기술 발전을 실시간으로 관찰할 수 있는 시각적 피드백을 제공하기 때문에",
                    correct: true,
                    explanation: "정확합니다! 튜토리얼은 이 시각적 측면을 카트폴이 만족스러운 이유로 강조합니다: '그래프에서 숫자가 개선되는 것을 보는 것이 아니라, AI가 당신의 눈앞에서 기술을 발전시키는 것을 지켜보고 있는 것입니다.' 이 즉각적이고 직관적인 피드백 루프가 학습 과정을 실체화합니다."
                },
                {
                    text: "보장된 최적 솔루션이 있는 유일한 강화학습 문제이기 때문에",
                    correct: false,
                    explanation: "카트폴은 다른 RL 문제에 비해 고유하게 보장된 최적 솔루션을 가지고 있지 않습니다. 많은 RL 작업은 최적 또는 준최적 솔루션을 가지고 있습니다. 카트폴의 가치는 다른 곳, 특히 직관적인 시각적 피드백에 있습니다."
                }
            ],
            hint: "튜토리얼에 따르면 카트폴이 특히 학습 예제로서 만족스러운 이유가 무엇인지 생각해 보세요."
        },
        {
            question: "카트폴 환경이 에이전트에게 막대의 균형을 유지하도록 장려하기 위해 어떤 보상 전략을 사용하나요?",
            options: [
                {
                    text: "막대가 완벽하게 수직일 때만 큰 양의 보상",
                    correct: false,
                    explanation: "환경은 특별히 완벽한 수직성에 보상을 주지 않습니다. 절대적인 완벽함을 추구하면 희소 보상 문제가 생겨 학습이 훨씬 더 어려워집니다."
                },
                {
                    text: "막대가 위에 있는 매 시간 단계마다 +1 보상, 떨어질 때 0",
                    correct: true,
                    explanation: "맞습니다! 코드는 `CalculateReward()`가 에피소드가 계속될 때 1을 반환하고, 끝났을 때 0을 반환함을 보여줍니다. 이 간단한 접근 방식은 강력한 인센티브를 만듭니다: 막대가 균형을 유지하는 시간이 길수록 에이전트가 받는 총 보상이 많아져 자연스럽게 균형 유지를 마스터하도록 장려합니다."
                },
                {
                    text: "막대가 수직에 얼마나 가까운지에 기반한 단계적 보상(더 수직에 가까울수록 더 높은 보상)",
                    correct: false,
                    explanation: "이 접근 방식이 작동할 수 있지만, 우리 구현에서는 이를 사용하지 않습니다. 우리 환경은 더 간단한 이진 보상을 사용합니다: 정확한 각도에 관계없이 살아남은 각 시간 단계에 +1, 에피소드가 끝날 때 0."
                }
            ],
            hint: "환경 코드의 `CalculateReward()` 메서드를 확인하여 정확히 어떤 보상이 언제 주어지는지 알아보세요."
        },
        {
            question: "maxStepsSoft와 maxStepsHard에 다른 값을 설정하는 목적은 무엇인가요?",
            options: [
                {
                    text: "에피소드를 조기에 종료하여 인위적으로 학습 속도를 높이기 위해",
                    correct: false,
                    explanation: "이것은 학습을 인위적으로 가속화하는 것에 관한 것이 아닙니다. 실제로 에피소드는 maxStepsHard까지 자연스러운 결론에 도달할 수 있습니다. 이 구분은 계산 효율성과 관련된 다른 목적을 위한 것입니다."
                },
                {
                    text: "자연스러운 환경 진행을 허용하면서 보상 계산을 제한하여 GPU 메모리 사용량을 줄이기 위해",
                    correct: true,
                    explanation: "그렇습니다! 튜토리얼이 설명하듯이, 이 기술을 통해 '자연스러운 환경 역학을 유지하면서 얼마나 많은 계산 노력을 투자할지 정확하게 제어할 수 있습니다.' maxStepsSoft까지만 보상과 그래디언트를 누적하고, 시뮬레이션은 maxStepsHard까지 자연스럽게 계속되어 긴 에피소드의 메모리 사용량을 크게 줄입니다."
                },
                {
                    text: "에이전트가 더 긴 에피소드를 다루기 전에 먼저 짧은 에피소드를 학습하는 커리큘럼을 만들기 위해",
                    correct: false,
                    explanation: "커리큘럼 학습은 유효한 RL 기술이지만, 소프트/하드 단계 제한의 목적이 아닙니다. 이 매개변수들은 에피소드 길이를 점진적으로 증가시키지 않고, 자연스러운 환경 동작을 유지하면서 계산 리소스를 관리합니다."
                }
            ],
            hint: "에피소드가 매우 길어질 때 GPU 메모리에 어떤 일이 발생하는지, 그리고 이 매개변수 구성이 그 문제를 해결하는 데 어떻게 도움이 되는지 생각해 보세요."
        },
        {
            question: "RLMatrix의 PPO batchSize 매개변수 해석은 표준 구현과 어떻게 다른가요?",
            options: [
                {
                    text: "개별 단계가 아닌 모델 업데이트 전에 수집할 완전한 에피소드 수를 의미합니다",
                    correct: true,
                    explanation: "정확합니다! 튜토리얼은 이 차이점을 명시적으로 지적합니다: 'RLMatrix에서 batchSize는 다른 많은 구현에서처럼 개별 단계 수가 아닌, 모델 업데이트 전에 수집할 완전한 에피소드 수를 의미합니다.' 이는 훈련을 구성할 때 중요한 차이점입니다."
                },
                {
                    text: "신경망의 은닉층 크기를 결정합니다",
                    correct: false,
                    explanation: "배치 크기는 신경망 아키텍처를 결정하지 않습니다. RLMatrix에서 'width' 매개변수가 은닉층의 크기를 제어합니다. 배치 크기는 대신 학습 업데이트 전에 얼마나 많은 경험을 수집할지와 관련이 있습니다."
                },
                {
                    text: "에이전트를 평가하기 전에 수행할 훈련 단계 수를 제어합니다",
                    correct: false,
                    explanation: "이것은 RLMatrix의 PPO 구현에서 배치 크기의 의미가 아닙니다. 배치 크기는 특별히 평가 일정이 아닌 학습을 위한 데이터 수집과 관련이 있습니다."
                }
            ],
            hint: "튜토리얼에는 RLMatrix의 PPO 구현 차이점을 설명하는 특정 섹션이 있습니다 - 배치 크기 해석에 대해 무엇이라고 말하는지 확인하세요."
        },
        {
            question: "훈련이 진행됨에 따라 에이전트의 행동에서 어떤 변화를 기대할 수 있나요?",
            options: [
                {
                    text: "에이전트가 무작위처럼 보이지만 균형을 유지하는 점점 더 복잡한 움직임 패턴을 개발합니다",
                    correct: false,
                    explanation: "성공적인 에이전트는 일반적으로 무작위처럼 보이는 움직임을 개발하지 않습니다. 진행은 복잡하거나 혼란스러운 패턴보다는 미묘하고 의도적인 제어 쪽으로 향합니다."
                },
                {
                    text: "에이전트가 무작위 움직임에서 반응적 수정을 거쳐 선제적 조정으로 발전합니다",
                    correct: true,
                    explanation: "튜토리얼에서 설명한 대로입니다! 에이전트는 이러한 진행을 따릅니다: 무작위 움직임 → 반응적 수정(막대가 이미 떨어지고 있을 때) → 더 일찍 개입 → 미묘한 선제적 조정. 이는 단순히 반응하는 것이 아니라 문제를 예상하는 방법을 배우는 것을 보여줍니다."
                },
                {
                    text: "에이전트가 항상 카트를 화면 중앙에 완벽하게 유지하는 법을 배웁니다",
                    correct: false,
                    explanation: "카트를 중앙에 유지하는 것이 반드시 최적의 전략은 아닙니다. 목표는 막대의 균형을 유지하는 것이며, 이는 카트를 전략적으로 이동시키는 것을 포함할 수 있습니다. 완벽한 중앙 유지는 예상되는 행동 진화의 일부로 언급되지 않습니다."
                }
            ],
            hint: "튜토리얼은 에이전트가 학습함에 따라 관찰할 수 있는 특정 행동 진화를 개략적으로 설명합니다. 이 변화를 설명하는 번호가 매겨진 목록을 찾아보세요."
        }
    ]}
/>

## 다음 단계

이 튜토리얼에서 당신은:
- 강화학습을 위한 실시간 물리 시뮬레이션을 설정했습니다
- 고전적인 제어 문제를 마스터하기 위한 완전한 에이전트를 구현했습니다
- 소프트/하드 종료 기법으로 메모리를 효율적으로 관리하는 방법을 배웠습니다
- RLMatrix의 PPO 구현이 표준과 어떻게 다른지 이해했습니다

다음으로, 툴킷을 사용하지 않고 동일한 환경을 구현하여 우리가 사용한 깔끔한 속성 뒤에서 무슨 일이 일어나고 있는지에 대한 통찰력을 얻을 것입니다.

<LinkCard
    title="툴킷 없는 카트폴"
    description="툴킷 추상화 없이 카트폴을 구현하여 내부에서 무슨 일이 일어나는지 확인해 보세요."
    href="/beginner/cartpolenotoolkit/"
/>