---
title: RLMatrix 시작하기
description: C#을 활용한 강화학습 초보자 가이드.
---
import Quiz from '@/components/Quiz.astro';

:::tip[RL 경험이 있으신가요?]
이미 강화학습 개념에 익숙하시다면, 더 빠른 설정을 위해 [빠른 시작 가이드](../quickstart/setup)를 확인해보세요.
:::

## 소개

전통적인 프로그램을 작성할 때, 우리는 컴퓨터에게 모든 상황에서 정확히 무엇을 할지 알려줍니다. 예를 들어, 숫자가 일치하는지 확인하는 프로그램을 작성하려면 다음과 같이 작성할 수 있습니다:

```csharp
if (input == pattern)
{
    return "정답입니다!";
}
else 
{
    return "다시 시도하세요!";
}
```

하지만 프로그램이 스스로 학습하길 원한다면 어떨까요? 규칙이 너무 복잡해서 작성하기 어렵거나, 우리 자신도 규칙을 완전히 알지 못한다면 어떨까요? 이럴 때 강화학습이 필요합니다.

:::note[강화학습이란 무엇인가요?]
<details>
    <summary>간단한 설명</summary>

    새로운 비디오 게임을 배우는 방법을 생각해보세요:
    1. 어떤 일이 일어나는지 보기 위해 컨트롤을 시도합니다
    2. 게임이 어떻게 반응하는지 봅니다
    3. 점수를 얻거나 생명을 잃습니다
    4. 시간이 지남에 따라 가장 효과적인 방법을 배웁니다

    강화학습도 같은 패턴을 따릅니다:
    1. AI(우리는 이를 "에이전트"라고 부릅니다)가 다양한 행동을 시도합니다
    2. 환경에서 어떤 일이 발생하는지 관찰합니다
    3. 보상이나 벌칙을 받습니다
    4. 시간이 지남에 따라 가장 효과적인 행동을 배웁니다

    아무도 AI에게 정확히 무엇을 해야 하는지 알려주지 않습니다 - AI는 시행착오를 통해 스스로 알아냅니다.
</details>
:::

## 프로젝트 설정하기
이 [GitHub 저장소](https://github.com/asieradzk/RLMatrixGettingStartedExample1)를 클론하여 따라할 수 있습니다.
먼저 모든 것을 설치해 봅시다:

```bash title="NuGet을 통해 RLMatrix 설치하기"
dotnet add package RLMatrix
dotnet add package RLMatrix.Toolkit
```

:::caution[하드웨어 요구 사항]
RLMatrix는 NVIDIA GPU가 있는 Windows PC에서만 테스트되었습니다. 그러나 이것은 필수 사항이 아니며, 많은 사용 사례에서 CPU에서의 훈련 및 추론이 적절하거나 더 빠를 수 있습니다.

호환되는 그래픽 카드가 없는 경우:
1. [RLMatrix 저장소](https://github.com/asieradzk/RL_Matrix)에서 소스 코드를 가져옵니다
2. CPU를 사용하도록 변경합니다(RLMatrix.csproj에서 `TorchSharp-cuda-windows` 찾기)
3. 직접 빌드합니다
:::

## 첫 번째 학습 환경

간단하지만 의미 있는 환경을 만들어 봅시다 - AI가 패턴을 일치시키는 방법을 배울 수 있는 환경입니다. 이것은 기본적으로 보이지만(직접 프로그래밍하기 쉬움), 필요한 모든 핵심 개념을 소개합니다.

:::note[구성 요소]
<details>
    <summary>주요 용어 설명</summary>

    시작하기 전에, 몇 가지 중요한 용어를 이해해 봅시다:

    - **환경(Environment)**: AI가 존재하는 세계입니다. 게임 보드나 시뮬레이션과 같습니다.

    - **상태/관찰(State/Observation)**: AI가 환경에 대해 볼 수 있거나 알 수 있는 것입니다.
    예: 일치시켜야 하는 현재 패턴.

    - **행동(Action)**: AI가 할 수 있는 것입니다.
    예: 숫자 선택하기.

    - **보상(Reward)**: AI에게 얼마나 잘하고 있는지 알려주는 피드백입니다.
    예: 정확한 일치에 +1, 잘못된 일치에 -1.

    - **에피소드(Episode)**: 태스크의 한 번의 완전한 시도입니다.
    게임의 한 라운드와 같이 생각하세요.

</details>
:::

다음은 완전한 환경 코드입니다:

```csharp title="PatternMatchingEnvironment.cs"
using RLMatrix.Toolkit;

namespace PatternMatchingExample;

[RLMatrixEnvironment]
public partial class PatternMatchingEnvironment
{
    private int pattern = 0;
    private int aiChoice = 0;
    private bool roundFinished = false;

    // 마지막 50단계에 대한 간단한 카운터
    private int correct = 0;
    private int total = 0;

    // 간단한 정확도 계산
    public float RecentAccuracy => total > 0 ? (float)correct / total * 100 : 0;

    [RLMatrixObservation]
    public float SeePattern() => pattern;

    [RLMatrixActionDiscrete(2)]
    public void MakeChoice(int choice)
    {
        aiChoice = choice;
        roundFinished = true;

        // 카운터 업데이트
        total++;
        if (aiChoice == pattern) correct++;
    }

    [RLMatrixReward]
    public float GiveReward() => aiChoice == pattern ? 1.0f : -1.0f;

    [RLMatrixDone]
    public bool IsRoundOver() => roundFinished;

    [RLMatrixReset]
    public void StartNewRound()
    {
        pattern = Random.Shared.Next(2);
        aiChoice = 0;
        roundFinished = false;
    }

    public void ResetStats()
    {
        correct = 0;
        total = 0;
    }
}
```

:::note[코드 이해하기]
<details>
    <summary>코드 분석</summary>

    각 부분을 살펴봅시다:

    **변수들:**
    ```csharp
    private int pattern = 0;      // 일치시킬 숫자
    private int aiChoice = 0;     // AI의 추측
    private bool roundFinished = false;  // 라운드 상태
    ```
    이것들은 환경에서 무슨 일이 일어나고 있는지 추적합니다.

    **특별한 속성들:**
    - `[RLMatrixEnvironment]`: RLMatrix에게 "이것은 학습 환경입니다"라고 알려줍니다
    - `[RLMatrixObservation]`: "AI가 볼 수 있는 것입니다"
    - `[RLMatrixActionDiscrete]`: "AI가 할 수 있는 선택들입니다"
    - `[RLMatrixReward]`: "AI의 성능을 이렇게 평가합니다"
    - `[RLMatrixReset]`: "새로 시작하는 방법입니다"

    툴킷은 이러한 속성을 사용하여 필요한 코드를 자동으로 생성합니다.
</details>
:::

## AI 훈련하기

이제 흥미로운 부분이 시작됩니다 - AI에게 패턴 일치를 가르치는 것입니다. 우리는 DQN(Deep Q-Network)이라는 알고리즘을 사용할 것입니다. 이름에 너무 신경 쓰지 마세요 - 이것은 AI에게 결정을 내리는 방법을 가르치는 한 가지 방법일 뿐입니다.

:::note[훈련 옵션]
<details>
    <summary>훈련 설정 이해하기</summary>

    AI가 어떻게 학습할지 구성해야 합니다:

    - `batchSize`: 한 번에 학습할 경험의 수
    여러 과거 시도를 함께 검토하는 것과 같습니다.

    - `memorySize`: 기억할 과거 경험의 수
    무엇이 작동하고 무엇이 작동하지 않았는지에 대한 노트북을 유지하는 것과 같습니다.

    - `gamma`: 미래 보상을 얼마나 중요하게 생각할지
    높은 값(1에 가까울수록) AI가 장기적으로 생각하게 합니다.

    - `epsStart`와 `epsEnd`: 탐색과 활용 사이의 균형
    새로운 전략을 시도하는 것과 잘 작동하는 것을 고수하는 것의 균형입니다.

    모든 매개변수와 그 효과에 대한 자세한 설명은 [매개변수 참조 가이드](../../reference/hyperparameters)를 참조하세요.
</details>
:::

다음은 훈련을 설정하는 방법입니다:

```csharp title="Program.cs"
using RLMatrix.Agents.Common;
using RLMatrix;
using PatternMatchingExample;

Console.WriteLine("패턴 일치 훈련을 시작합니다...\n");

// AI가 학습하는 방법 설정
var learningSetup = new DQNAgentOptions(
    batchSize: 32,      // 한 번에 32개의 경험으로부터 학습
    memorySize: 1000,   // 마지막 1000번의 시도를 기억
    gamma: 0.99f,       // 미래 보상을 많이 고려
    epsStart: 1f,       // 모든 것을 시도하며 시작
    epsEnd: 0.05f,      // 결국 잘 작동하는 것을 고수
    epsDecay: 150f      // 전환 속도
);

// 환경 생성
var environment = new PatternMatchingEnvironment().RLInit();
var env = new List<IEnvironmentAsync<float[]>> { 
    environment,
    //new PatternMatchingEnvironment().RLInit() //병렬로 훈련할 환경을 더 추가할 수 있습니다
};

// 학습 에이전트 생성
var agent = new LocalDiscreteRolloutAgent<float[]>(learningSetup, env);

// 학습 시작!
for (int i = 0; i < 1000; i++)
{
    await agent.Step();

    if ((i + 1) % 50 == 0)
    {
        Console.WriteLine($"스텝 {i + 1}/1000 - 최근 50 스텝 정확도: {environment.RecentAccuracy:F1}%");
        environment.ResetStats();
        
        Console.WriteLine("\n계속하려면 Enter를 누르세요...");
        Console.ReadLine();
    }
}

Console.WriteLine("\n훈련 완료!");
Console.ReadLine();
```

이 코드를 실행하면 50단계마다 훈련 진행 상황이 표시됩니다:

```bash title="훈련 진행 상황"
패턴 일치 훈련을 시작합니다...

스텝 50/1000 - 최근 50 스텝 정확도: 48.0%
계속하려면 Enter를 누르세요...

스텝 100/1000 - 최근 50 스텝 정확도: 68.0%
계속하려면 Enter를 누르세요...

스텝 150/1000 - 최근 50 스텝 정확도: 86.0%
계속하려면 Enter를 누르세요...

스텝 200/1000 - 최근 50 스텝 정확도: 82.0%
계속하려면 Enter를 누르세요...
```

:::tip[예상되는 결과]
훈련 진행 상황을 지켜보면 AI가 개선되는 것을 볼 수 있습니다:

1. 처음에는 약 50% 정확도(무작위 추측)
2. 학습함에 따라 정확도가 꾸준히 향상
3. 결국 80-90% 이상의 정확도 도달

50단계마다의 일시 중지는 무작위 추측에서 숙련된 일치로의 이 진행 과정을 관찰할 수 있게 해줍니다. 이것이 실제로 작동하는 강화학습입니다!
:::

## 단순 일치를 넘어서

우리의 예제는 간단하지만, 동일한 원칙이 훨씬 더 복잡한 문제에도 적용됩니다:

:::note[실제 응용 분야]
<details>
    <summary>이것이 어디로 이어질 수 있을까요</summary>

    여기서 사용한 같은 기본 구조는 다음과 같은 분야로 확장됩니다:
    - 게임 플레이 AI
    - 로봇 제어
    - 자원 관리
    - 교통 최적화

    주요 차이점은 기본적인 접근 방식이 아니라 상태와 행동의 복잡성입니다.
</details>
:::

## 이해도 테스트
<Quiz
    title="강화학습 기초 이해하기"
    questions={[
        {
            question: "어떤 작업에 대해 전통적인 프로그래밍 대신 강화학습을 선택하는 이유는 무엇인가요?",
            options: [
                {
                    text: "프로그램이 극도의 정밀도로 작동해야 할 때",
                    correct: false,
                    explanation: "사실, 우리가 정확히 원하는 것을 알고 있을 때 전통적인 프로그래밍은 정밀도에서 뛰어납니다. 강화학습은 규칙이 복잡하거나 알려지지 않은 시나리오에서 빛을 발하며, 반드시 최대 정밀도가 목표일 때만은 아닙니다."
                },
                {
                    text: "규칙이 너무 복잡해서 수동으로 프로그래밍하기 어렵거나 우리 자신이 완전히 알지 못할 때",
                    correct: true,
                    explanation: "정확히 맞습니다! 강화학습은 규칙이 너무 복잡해서 명시하기 어려울 때(로봇 균형 잡기 같은) 또는 우리가 최적의 접근 방식을 완전히 이해하지 못할 때 특히 가치가 있습니다. AI는 명시적으로 프로그래밍되기보다는 경험을 통해 해결책을 발견할 수 있습니다."
                },
                {
                    text: "프로그램이 전통적인 코드보다 더 빠르게 실행되어야 할 때",
                    correct: false,
                    explanation: "강화학습은 실행 속도에 관한 것이 아닙니다 - 사실, 전통적인 프로그래밍이 보통 더 빠르게 실행됩니다. RL은 모든 상황에 대해 명시적으로 코딩되기보다는 경험으로부터 학습하는 프로그램에 관한 것입니다."
                }
            ],
            hint: "전통적인 if/else 프로그래밍의 한계와 시스템이 시행착오를 통해 패턴을 발견하도록 하는 것 사이의 차이점을 생각해보세요."
        },
        {
            question: "우리 예제에서 epsStart를 1.0으로, epsEnd를 0.05와 같은 낮은 값으로 설정하는 것이 왜 중요했나요?",
            options: [
                {
                    text: "에이전트가 항상 가장 높은 보상 행동을 선택하도록 보장합니다",
                    correct: false,
                    explanation: "그것은 목적이 아닙니다. 에이전트가 항상 최선이라고 생각하는 것을 선택한다면(오직 활용만), 아직 시도하지 않은 잠재적으로 더 나은 전략을 발견하지 못할 것입니다."
                },
                {
                    text: "이 설정은 시간이 지남에 따라 에이전트의 학습률을 제어합니다",
                    correct: false,
                    explanation: "이 매개변수들은 시간이 지남에 따라 변하지만, 직접적으로 학습률을 제어하지는 않습니다(그것은 'lr' 매개변수입니다). 그들은 강화학습의 다른 기본적인 부분을 제어합니다."
                },
                {
                    text: "이것은 탐색(새로운 것 시도)과 활용(작동하는 것 사용) 사이의 균형을 만들어 시간이 지남에 따라 변화합니다",
                    correct: true,
                    explanation: "맞습니다! 이것은 고전적인 탐색-활용 균형입니다. epsStart: 1f로 시작함으로써, 에이전트는 처음에 모든 것을 시도합니다(순수 탐색). 훈련이 진행됨에 따라, 점차 epsEnd: 0.05f로 이동하여, 대부분 학습된 것 중 가장 잘 작동하는 것을 사용하면서(주로 활용) 가끔씩 탐색합니다."
                }
            ],
            hint: "훈련 초기와 후기에 어떤 일이 일어나는지 생각해보세요 - 에이전트의 행동이 어떻게 변하고, 왜 그것이 중요한가요?"
        },
        {
            question: "보상 함수를 정확한 일치에 대해서는 +1을 주지만 부정확한 일치에 대해서는 벌칙을 주지 않도록 변경하면 어떤 일이 발생할까요?",
            options: [
                {
                    text: "에이전트가 긍정적인 피드백만 받기 때문에 학습이 더 빨라질 것입니다",
                    correct: false,
                    explanation: "벌칙 없이는 에이전트가 실제로 더 느리게 학습하거나 전혀 학습하지 못할 수 있습니다. 긍정적인 보상만 있으면 무작위 추측도 여전히 50%의 확률로 보상을 받게 되어, 무작위 확률 이상으로 개선할 이유가 거의 없습니다."
                },
                {
                    text: "에이전트가 잘못된 행동에 대한 명확한 피드백을 받지 못하기 때문에 학습이 느려지거나 실패할 것입니다",
                    correct: true,
                    explanation: "정확합니다! 이것은 잘 설계된 보상 함수의 중요성을 강조합니다. 잘못된 일치에 대한 벌칙 없이는 에이전트가 실수를 할 때 옳고 그름을 구분하는 피드백을 받지 못합니다. 여전히 절반의 시간에 보상을 받기 때문에 무작위 추측이 충분히 좋다고 결론지을 수 있습니다."
                },
                {
                    text: "에이전트는 동일한 패턴을 학습하지만 경험을 저장하기 위해 더 많은 메모리가 필요할 것입니다",
                    correct: false,
                    explanation: "메모리 요구 사항은 보상 구조와 직접적으로 관련이 없습니다. 여기서 핵심 문제는 에이전트가 얼마나 많은 메모리를 사용하는지가 아니라 에이전트가 받는 학습 신호의 품질입니다."
                }
            ],
            hint: "학습을 동기 부여하는 것이 무엇인지 생각해보세요 - 단순히 보상을 받는 것인가요, 아니면 벌칙을 피하는 것도 포함되나요?"
        },
        {
            question: "gamma(우리 예제에서 0.99f로 설정)가 학습 과정에서 어떤 역할을 하나요?",
            options: [
                {
                    text: "에이전트가 한 번에 기억할 수 있는 패턴의 수를 결정합니다",
                    correct: false,
                    explanation: "패턴 기억 용량은 주로 신경망 아키텍처와 관련이 있으며, gamma 매개변수와는 관련이 없습니다. Gamma는 에이전트가 보상을 평가하는 방식에서 다른 목적을 가집니다."
                },
                {
                    text: "에이전트가 즉각적인 보상과 잠재적인 미래 보상을 얼마나 중요하게 여기는지 제어합니다",
                    correct: true,
                    explanation: "맞습니다! Gamma는 에이전트가 미래 보상을 즉각적인 것과 비교하여 어떻게 평가하는지 결정하는 할인 요소입니다. 0.99f라는 높은 설정으로, 에이전트는 미래 보상을 즉각적인 것만큼 거의 중요하게 여겨, 장기적으로 좋은 결과를 가져오는 전략을 학습하도록 장려합니다."
                },
                {
                    text: "에이전트가 성공하지 못한 시도를 얼마나 빨리 잊는지 설정합니다",
                    correct: false,
                    explanation: "에이전트의 과거 경험 기억은 gamma가 아니라 memorySize 매개변수에 의해 제어됩니다. Gamma는 에이전트가 시간 경과에 따른, 행동의 가치를 어떻게 평가하는지에 영향을 미칩니다."
                }
            ],
            hint: "더 복잡한 환경에서는 행동이 항상 즉각적인 보상으로 이어지지 않습니다. 에이전트가 지금 작은 보상과 나중에 잠재적으로 더 큰 보상 사이에서 어떻게 결정을 내릴까요?"
        },
        {
            question: "배운 내용을 바탕으로, 다음 중 강화학습 접근 방식에 가장 적합한 작업은 무엇인가요?",
            options: [
                {
                    text: "숫자 목록을 오름차순으로 정렬하기",
                    correct: false,
                    explanation: "정렬은 이미 최적의 알고리즘이 알려진 잘 이해된 문제입니다. 어떤 입력에 대해서도 정확한 출력이 무엇인지 정확히 알고 있으므로 전통적인 프로그래밍이 더 적합합니다."
                },
                {
                    text: "복잡한 관절 역학을 가진 시뮬레이션된 로봇의 균형 잡기",
                    correct: true,
                    explanation: "완벽한 선택입니다! 로봇 균형 잡기는 정확하게 모델링하기 어려운 복잡한 물리학을 포함하며, 균형을 유지하기 위한 많은 잠재적인 전략이 있습니다. 이것은 규칙이 복잡하고 최적의 정책이 인간에게도 명확하지 않을 때 RL이 빛을 발하는 예입니다."
                },
                {
                    text: "섭씨와 화씨 간의 온도 변환",
                    correct: false,
                    explanation: "이것은 전통적인 프로그래밍으로 쉽게 구현할 수 있는 간단한 수학 공식(F = C × 9/5 + 32)입니다. 각 입력에 대해 단 하나의 정확한 답이 있어, 이 작업에 강화학습을 사용하는 것은 불필요하게 복잡합니다."
                }
            ],
            hint: "명시적으로 지정하기 어렵지만 시행착오를 통해 배울 수 있는 규칙을 가진 작업이 무엇인지 생각해보세요."
        }
    ]}
/>

## 다음 단계

더 나아갈 준비가 되셨나요? 다음 단계로 갈 수 있습니다:
- [대시보드 사용 방법](../beginner/howtodashboard)
- [연속적인 행동 다루기](../beginner/continuousactions)
- [Cart-Pole 예제](../beginner/cartpole)

우리는 두 가지 주요 알고리즘을 사용할 수 있습니다:
- DQN: 방금 사용한 것으로, 단순한 선택에 좋으며 큰 리플레이 메모리의 이점을 누립니다.
- PPO: 더 고급으로, 연속적인 행동(속도나 방향 제어와 같은)을 처리합니다.

:::note[진행 상황 이해하기]
<details>
    <summary>핵심 요점</summary>

    배운 내용:
    1. 강화학습이 전통적인 프로그래밍과 어떻게 다른지
    2. 기본 학습 환경을 만드는 방법
    3. 훈련을 설정하고 실행하는 방법
    4. 더 복잡한 응용 프로그램을 위한 구성 요소

    가장 중요한 것은, 명시적인 지시 대신에 경험을 통해 컴퓨터를 가르칠 수 있다는 것을 보았습니다.
</details>
:::