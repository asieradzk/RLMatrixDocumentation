---
title: Getting Started with RLMatrix
description: A beginner-friendly introduction to reinforcement learning with C#.
---
import Quiz from '@/components/Quiz.astro';

:::tip[Experienced with RL?]
If you're already familiar with reinforcement learning concepts, check out our [Quickstart Guide](../quickstart/setup) for a faster setup.
:::

## Introduction

When we write traditional programs, we tell the computer exactly what to do in every situation. For example, if we wanted to write a program that matches numbers, we might write:

```csharp
if (input == pattern)
{
    return "Correct!";
}
else 
{
    return "Try again!";
}
```

But what if we want our program to learn on its own? What if the rules are too complex to write out, or we don't even know the rules ourselves? This is where reinforcement learning comes in.

:::note[What is Reinforcement Learning?]
<details>
    <summary>A Simple Explanation</summary>

    Think about how you might learn to play a new video game:
    1. You try some controls to see what happens
    2. You see how the game responds
    3. You get points or lose lives
    4. Over time, you learn what works best

    Reinforcement learning follows the same pattern:
    1. The AI (we call it an "agent") tries different actions
    2. It sees what happens in its environment
    3. It gets rewards or penalties
    4. Over time, it learns what actions work best

    No one tells the AI exactly what to do - it figures it out through trial and error.
</details>
:::

## Setting Up Your Project
You can follow along or clone this [GitHub repository](https:/www.github.com/RLMatrix/RLMatrix-Example).
First, let's get everything installed:

```bash title="Installing RLMatrix via NuGet"
dotnet add package RLMatrix
dotnet add package RLMatrix.Toolkit
```

:::caution[Hardware Requirements]
RLMatrix was only tested on Windows PCs with an NVIDIA GPU. This is however not necessary and for many use-cases training and inference on the CPU will be adequate or even faster.

If you don't have a compatible graphics card, you can:
1. Get the source code from [RLMatrix repository](https://github.com/asieradzk/RL_Matrix)
2. Change it to use your CPU instead (look for `TorchSharp-cuda-windows` in RLMatrix.csproj)
3. Build it yourself
:::

## Your First Learning Environment

Let's create something simple but meaningful - an environment where our AI will learn to match patterns. While this seems basic (and would be trivial to program directly), it introduces all the key concepts we need.

:::note[Building Blocks]
<details>
    <summary>Key Terms Explained</summary>

    Before we dive in, let's understand some important terms:

    - **Environment**: The world our AI lives in. Like a game board or simulation.

    - **State/Observation**: What our AI can see or know about its environment.
    For example: The current pattern it needs to match.

    - **Action**: Something our AI can do.
    For example: Choosing a number.

    - **Reward**: Feedback telling our AI how well it's doing.
    For example: +1 for correct match, -1 for wrong match.

    - **Episode**: One complete attempt at the task.
    Think of it like one round of a game.

</details>
:::

Here's our complete environment:

```csharp title="PatternMatchingEnvironment.cs"
using RLMatrix.Toolkit;

namespace PatternMatchingExample;

[RLMatrixEnvironment]
public partial class PatternMatchingEnvironment
{
    private int pattern = 0;
    private int aiChoice = 0;
    private bool roundFinished = false;

    // Simple counters for last 50 steps
    private int correct = 0;
    private int total = 0;

    // Simple accuracy calculation
    public float RecentAccuracy => total > 0 ? (float)correct / total * 100 : 0;

    [RLMatrixObservation]
    public float SeePattern() => pattern;

    [RLMatrixActionDiscrete(2)]
    public void MakeChoice(int choice)
    {
        aiChoice = choice;
        roundFinished = true;

        // Update counters
        total++;
        if (aiChoice == pattern) correct++;
    }

    [RLMatrixReward]
    public float GiveReward() => aiChoice == pattern ? 1.0f : -1.0f;

    [RLMatrixDone]
    public bool IsRoundOver() => roundFinished;

    [RLMatrixReset]
    public void StartNewRound()
    {
        pattern = Random.Shared.Next(2);
        aiChoice = 0;
        roundFinished = false;
    }

    public void ResetStats()
    {
        correct = 0;
        total = 0;
    }
}
```

:::note[Understanding the Code]
<details>
    <summary>Code Breakdown</summary>

    Let's look at each part:

    **The Variables:**
    ```csharp
    private int pattern = 0;      // The number to match
    private int aiChoice = 0;     // The AI's guess
    private bool roundFinished = false;  // Round status
    ```
    These keep track of what's happening in our environment.

    **The Special Attributes:**
    - `[RLMatrixEnvironment]`: Tells RLMatrix "this is a learning environment"
    - `[RLMatrixObservation]`: "This is what the AI can see"
    - `[RLMatrixActionDiscrete]`: "These are the choices the AI can make"
    - `[RLMatrixReward]`: "This is how we score the AI's performance"
    - `[RLMatrixReset]`: "This is how we start fresh"

    The toolkit uses these attributes to automatically generate the necessary code.
</details>
:::

## Training Your AI

Now comes the interesting part - teaching our AI to match patterns. We'll use an algorithm called DQN (Deep Q-Network). Don't worry too much about the name - it's just one way of teaching AI to make decisions.

:::note[Training Options]
<details>
    <summary>Understanding Training Settings</summary>

    We need to configure how our AI learns:

    - `batchSize`: How many experiences to learn from at once
    Think of it like reviewing multiple past attempts together.

    - `memorySize`: How many past experiences to remember
    Like keeping a notebook of what worked and what didn't.

    - `gamma`: How much to care about future rewards
    Higher values (closer to 1) make the AI think long-term.

    - `epsStart` and `epsEnd`: How much to explore vs use what's known
    Like trying new strategies vs sticking to what works.

    For detailed explanations of all parameters and their effects, see our [Parameter Reference Guide](../../reference/hyperparameters).
</details>
:::

Here's how we set up the training:

```csharp title="Program.cs"
using RLMatrix.Agents.Common;
using RLMatrix;
using PatternMatchingExample;

Console.WriteLine("Starting pattern matching training...\n");

// Set up how our AI will learn
var learningSetup = new DQNAgentOptions(
    batchSize: 32,      // Learn from 32 experiences at once
    memorySize: 1000,   // Remember last 1000 attempts
    gamma: 0.99f,       // Care a lot about future rewards
    epsStart: 1f,       // Start by trying everything
    epsEnd: 0.05f,      // Eventually stick to what works
    epsDecay: 150f      // How fast to transition
);

// Create our environment
var environment = new PatternMatchingEnvironment().RLInit();
var env = new List<IEnvironmentAsync<float[]>> { 
    environment,
    //new PatternMatchingEnvironment().RLInit() //you can add more than one to train in parallel
};

// Create our learning agent
var agent = new LocalDiscreteRolloutAgent<float[]>(learningSetup, env);

// Let it learn!
for (int i = 0; i < 1000; i++)
{
    await agent.Step();

    if ((i + 1) % 50 == 0)
    {
        Console.WriteLine($"Step {i + 1}/1000 - Last 50 steps accuracy: {environment.RecentAccuracy:F1}%");
        environment.ResetStats();
        
        Console.WriteLine("\nPress Enter to continue...");
        Console.ReadLine();
    }
}

Console.WriteLine("\nTraining complete!");
Console.ReadLine();
```

When you run this code, you'll see the training progress displayed every 50 steps:

```bash title="Training Progress"
Starting pattern matching training...

Step 50/1000 - Last 50 steps accuracy: 48.0%
Press Enter to continue...

Step 100/1000 - Last 50 steps accuracy: 68.0%
Press Enter to continue...

Step 150/1000 - Last 50 steps accuracy: 86.0%
Press Enter to continue...

Step 200/1000 - Last 50 steps accuracy: 82.0%
Press Enter to continue...
```

:::tip[What to Expect]
As you watch the training progress, you'll see your AI improve:

1. Around 50% accuracy at first (random guessing)
2. Steadily improving accuracy as it learns
3. Eventually reaching 80-90% accuracy or higher

The pauses every 50 steps let you observe this progression from random guesses to skilled matching. This is reinforcement learning in action!
:::

## Beyond Simple Matching

While our example is straightforward, the same principles apply to much more complex problems:

:::note[Real World Applications]
<details>
    <summary>Where This Can Lead</summary>

    The same basic structure we used here scales to:
    - Game playing AI
    - Robot control
    - Resource management
    - Traffic optimization

    The main difference is the complexity of states and actions, not the fundamental approach.
</details>
:::

## Test Your Understanding
<Quiz
    title="Understanding Reinforcement Learning Basics"
    questions={[
        {
            question: "Why would we choose reinforcement learning over traditional programming for a task?",
            options: [
                {
                    text: "When we need the program to work with extreme precision",
                    correct: false,
                    explanation: "Actually, traditional programming often excels at precision when we know exactly what we want. Reinforcement learning shines in scenarios where the rules are complex or unknown, not necessarily when maximum precision is the goal."
                },
                {
                    text: "When the rules are too complex to manually program or we don't fully know them ourselves",
                    correct: true,
                    explanation: "Exactly right! Reinforcement learning is particularly valuable when the rules are too complex to specify (like balancing a robot) or when we don't fully understand the optimal approach ourselves. The AI can discover solutions through experience rather than being explicitly programmed."
                },
                {
                    text: "When we need the program to run faster than traditional code",
                    correct: false,
                    explanation: "Reinforcement learning isn't about speed of execution - in fact, traditional programming usually runs faster. RL is about having programs learn from experience rather than being explicitly coded for every situation."
                }
            ],
            hint: "Think about the limitations of traditional if/else programming versus letting a system discover patterns through trial and error."
        },
        {
            question: "In our example, why was it important to set epsStart to 1.0 and epsEnd to a lower value like 0.05?",
            options: [
                {
                    text: "This ensures the agent always picks the highest reward action",
                    correct: false,
                    explanation: "That's not quite the purpose. If the agent always picked what it thought was best (exploitation only), it would never discover potentially better strategies it hasn't tried yet."
                },
                {
                    text: "These settings control the agent's learning rate over time",
                    correct: false,
                    explanation: "While these parameters do change over time, they don't directly control the learning rate (that would be the 'lr' parameter). They control something else fundamental to reinforcement learning."
                },
                {
                    text: "This creates a balance between exploration (trying new things) and exploitation (using what works) that shifts over time",
                    correct: true,
                    explanation: "That's right! This is the classic exploration-exploitation balance. By starting with epsStart: 1f, the agent initially tries everything (pure exploration). As training progresses, it gradually shifts toward epsEnd: 0.05f, where it mostly uses what it's learned works best (mostly exploitation) while still occasionally exploring."
                }
            ],
            hint: "Consider what happens at the beginning of training versus later on - how does the agent's behavior change, and why is that important?"
        },
        {
            question: "What would likely happen if we changed our reward function to only give +1 for correct matches but no penalty for incorrect matches?",
            options: [
                {
                    text: "Learning would be faster because the agent would only receive positive feedback",
                    correct: false,
                    explanation: "Without penalties, the agent would actually learn more slowly or possibly not at all. With only positive rewards, random guessing still yields rewards 50% of the time, giving little incentive to improve beyond random chance."
                },
                {
                    text: "Learning would be slower or fail because the agent wouldn't receive clear feedback about incorrect actions",
                    correct: true,
                    explanation: "Exactly! This highlights the importance of well-designed reward functions. Without penalties for incorrect matches, the agent gets no feedback distinguishing wrong from right when it makes a mistake. It might conclude that random guessing is good enough since it still receives rewards half the time."
                },
                {
                    text: "The agent would learn the same pattern but would need more memory to store the experiences",
                    correct: false,
                    explanation: "Memory requirements aren't directly related to the reward structure. The key issue here is the quality of learning signals the agent receives, not how much memory it uses."
                }
            ],
            hint: "Think about what motivates learning - is it just receiving rewards, or is it also avoiding penalties?"
        },
        {
            question: "What role does gamma (set to 0.99f in our example) play in the learning process?",
            options: [
                {
                    text: "It determines how many patterns the agent can memorize at once",
                    correct: false,
                    explanation: "Pattern memorization capacity is primarily related to the neural network architecture, not the gamma parameter. Gamma serves a different purpose in how the agent evaluates rewards."
                },
                {
                    text: "It controls how much the agent values immediate rewards versus potential future rewards",
                    correct: true,
                    explanation: "Correct! Gamma is the discount factor that determines how the agent values future rewards compared to immediate ones. With our high setting of 0.99f, the agent cares almost as much about future rewards as immediate ones, encouraging it to learn strategies that lead to good outcomes in the long run."
                },
                {
                    text: "It sets how quickly the agent forgets unsuccessful attempts",
                    correct: false,
                    explanation: "The agent's memory of past experiences is controlled by the memorySize parameter, not gamma. Gamma influences how the agent evaluates the value of actions across time."
                }
            ],
            hint: "In more complex environments, actions don't always lead to immediate rewards. How would an agent decide between a small reward now versus potentially larger rewards later?"
        },
        {
            question: "Based on what you've learned, which of these tasks would be MOST suitable for a reinforcement learning approach?",
            options: [
                {
                    text: "Sorting a list of numbers in ascending order",
                    correct: false,
                    explanation: "Sorting is a well-understood problem with optimal algorithms already known. Traditional programming would be more appropriate here since we know exactly what the correct output should be for any input."
                },
                {
                    text: "Balancing a simulated robot that has complex joint dynamics",
                    correct: true,
                    explanation: "Perfect choice! Robot balancing involves complex physics that's difficult to model precisely, with many potential strategies for maintaining balance. This exemplifies when RL shines - when the rules are complex and the optimal policy isn't obvious even to humans."
                },
                {
                    text: "Converting temperature between Celsius and Fahrenheit",
                    correct: false,
                    explanation: "This is a straightforward mathematical formula (F = C Ã— 9/5 + 32) that's easily implemented with traditional programming. There's a single correct answer for each input, making reinforcement learning unnecessarily complex for this task."
                }
            ],
            hint: "Consider which task has rules that are difficult to specify explicitly but could be learned through trial and error."
        }
    ]}
/>

## Next Steps

Ready to go further? Your next steps could be:
- [How to use dashboard?](../beginner/howtodashboard)
- [Working with Continuous Actions](../beginner/continuousactions)
- [Cart-Pole Example](../beginner/cartpole)

We have two main algorithms available:
- DQN: What we just used, good for simple choices, benefits from large replay memory.
- PPO: More advanced, handles continuous actions (like controlling speed or direction)

:::note[Understanding Your Progress]
<details>
    <summary>Key Takeaways</summary>

    You've learned:
    1. How reinforcement learning differs from traditional programming
    2. How to create a basic learning environment
    3. How to set up and run training
    4. The building blocks for more complex applications

    Most importantly, you've seen how we can teach computers through experience rather than explicit instructions.
</details>
:::